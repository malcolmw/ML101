{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22b7ff49-867f-4c8d-ab06-55bb03b2db53",
   "metadata": {},
   "source": [
    "# ML101:  A short course on machine learning\n",
    "\n",
    "## Malcolm C. A. White and Nori Nakata\n",
    "\n",
    "*Machine learning* (ML) refers to a diverse set of computer algorithms that are designed to \"learn\" how to make inferences from input data. ML is a component of Artifical Intelligence (AI), the study of reasoned decision making in machines.\n",
    "\n",
    "In this short course, we will look at two main problems that ML algorithms attempt to solve: a) **classification** and b) **regression**. There are many other problems that ML algorithms attempt to solve, but we will limit ourselves to this pair of related problems for this short course. For our purposes, we will treat classification problems as those where we wish to infer the value of a categorical or discrete, numerical variable from a set of independent or explanatory variables. We will treat regression problems as those where we wish to infer the value of a continuous, numerical variable from a set of explanatory variables.\n",
    "\n",
    "In service of solutions to these classes of problems, we will investigate various *supervised learning* methods, including *Artificial Neural Networks* (ANN), for solving basic classification and regression problems. As an example of a *Dimensionality Reduction* technique, we will also look at *Principal Component Analysis (PCA)* as a tool for building Image Recognition algorithms.\n",
    "\n",
    "We begin by import various packages we will need.  Most notably, we will be using [`sckit-learn`](https://scikit-learn.org/stable/index.html) (`sklearn` for short) to build our ML models. Much of the code in this notebook is modeled after code in the `sklearn` documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c05600a-fc43-491d-8373-4199485b2b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.cluster\n",
    "import sklearn.datasets\n",
    "import sklearn.decomposition\n",
    "import sklearn.linear_model\n",
    "import sklearn.metrics\n",
    "import sklearn.mixture\n",
    "import sklearn.neighbors\n",
    "import sklearn.neural_network\n",
    "import sklearn.svm\n",
    "import sklearn.tree\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8d83f0-752e-4144-880c-b78023358698",
   "metadata": {},
   "source": [
    "We also need to define some functions to summarize and visualize our data and analysis results. The code cell below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfda51d-7990-4eff-829a-234ebf1994df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_report(X, y, clf, title=None):\n",
    "    print(\n",
    "        sklearn.metrics.classification_report(\n",
    "            y, \n",
    "            clf.predict(X), \n",
    "            labels=clf.classes_)\n",
    "    )\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    sns.heatmap(\n",
    "        sklearn.metrics.confusion_matrix(y, clf.predict(X), normalize=\"true\"), \n",
    "        annot=True,\n",
    "        ax=ax,\n",
    "        xticklabels=clf.classes_,\n",
    "        yticklabels=clf.classes_\n",
    "    )\n",
    "    ax.set_xlabel(\"Predicted label\")\n",
    "    ax.set_ylabel(\"True label\")\n",
    "    if title is not None:\n",
    "        fig.suptitle(title)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "def plot_decision_function(X, y, clf, title=None, palette=\"icefire\"):\n",
    "    _palette = sns.color_palette(palette)\n",
    "    for i in range(1, len(_palette)-1):\n",
    "        _palette.remove(_palette[1])\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    sns.scatterplot(\n",
    "        data=X.merge(y, left_index=True, right_index=True), \n",
    "        x=\"feature 1\",\n",
    "        y=\"feature 2\",\n",
    "        hue=\"target\",\n",
    "        hue_order=sorted(y.unique()),\n",
    "        palette=_palette,\n",
    "        ax=ax\n",
    "    )\n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(*ax.get_xlim()),\n",
    "        np.linspace(*ax.get_ylim())\n",
    "    )\n",
    "    xy = np.stack([xx.flatten(), yy.flatten()]).T\n",
    "    z = clf.decision_function(xy)\n",
    "\n",
    "    ax.pcolormesh(\n",
    "        xx,\n",
    "        yy,\n",
    "        z.reshape(xx.shape),\n",
    "        zorder=0,\n",
    "        cmap=sns.color_palette(palette, as_cmap=True),\n",
    "        shading=\"gouraud\"\n",
    "    )\n",
    "    \n",
    "\n",
    "def plot_gallery(images, h, w, titles=None, nrows=3, ncols=4):\n",
    "    figsize = (1.5*ncols, 1.5*nrows*h/w)\n",
    "    fig, axes = plt.subplots(figsize=figsize, nrows=nrows, ncols=ncols)\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n",
    "        if titles is not None:\n",
    "            ax.set_title(titles[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "def plot_joint(X, y, palette=\"icefire\", nclasses=2):\n",
    "    if nclasses == 2:\n",
    "        _palette = sns.color_palette(palette)\n",
    "        for i in range(1, len(_palette)-1):\n",
    "            _palette.remove(_palette[1])\n",
    "    else:\n",
    "        _palette = palette\n",
    "        \n",
    "    g = sns.JointGrid(\n",
    "        data=X.merge(y, left_index=True, right_index=True), \n",
    "        x=\"feature 1\",\n",
    "        y=\"feature 2\",\n",
    "        hue=\"target\",\n",
    "        hue_order=sorted(y.unique()),\n",
    "        palette=_palette,\n",
    "        space=0\n",
    "    )\n",
    "    g.plot_joint(\n",
    "        sns.scatterplot,\n",
    "    )\n",
    "    g.plot_marginals(\n",
    "        sns.histplot, \n",
    "        kde=True,\n",
    "        alpha=1/2,\n",
    "        bins=32\n",
    "    )\n",
    "    \n",
    "\n",
    "def plot_pairs(X, y, model=None, title=None):\n",
    "    fig, axes = plt.subplots(\n",
    "        figsize=(12, 6), \n",
    "        nrows=2, \n",
    "        ncols=4, \n",
    "        sharey=True\n",
    "    )\n",
    "    i = 0\n",
    "    for column, ax in zip(sorted(X.columns), axes.flatten()):\n",
    "        sns.scatterplot(\n",
    "            x=X[column],\n",
    "            y=y,\n",
    "            s=16,\n",
    "            ax=ax\n",
    "        )\n",
    "        if model is not None:\n",
    "            sns.scatterplot(\n",
    "                x=X[column],\n",
    "                y=model.predict(X),\n",
    "                s=16,\n",
    "                ax=ax\n",
    "            )\n",
    "        xmin, xmax = X[column].quantile([0.01, 0.99])\n",
    "        dx = 0.02 * (xmax - xmin)\n",
    "        ax.set_xlim(xmin-dx, xmax+dx)\n",
    "            \n",
    "    if title is not None:\n",
    "        if model is not None:\n",
    "            sqerr = sklearn.metrics.mean_squared_error(\n",
    "                y_test, \n",
    "                model.predict(X_test)\n",
    "            )\n",
    "            fig.text(0.98, 0.98, f\"$\\epsilon={sqerr:.3f}$\", va=\"top\", ha=\"right\")\n",
    "        fig.suptitle(title)\n",
    "    if model is not None:\n",
    "        handles = axes[0, 0].get_children()[:2]\n",
    "        labels = [\"True\", \"Predicted\"]\n",
    "        fig.legend(handles, labels, loc=2)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return (fig, axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6548651a-cc27-48e7-8c08-e12caefcd070",
   "metadata": {},
   "source": [
    "# **1. Classification (Supervised)**\n",
    "\n",
    "In this section on Classification, we will compare the performance of three classification algorithms: a) k-Nearest Neighbours, b) Support Vector Machines, and c) Random Forests.\n",
    "\n",
    "## **1.1 Data**\n",
    "\n",
    "To build and test our different classification models, we will work with a 2-D, synthetic data set with two classes. In other words, we want to perform binary classification using just two features. The methods we will look at generalize to data from multiple classes with more than two features, although the computational costs grow at different rates for the various algorithms. It is simplest to visualize data with only two classes in two dimensions, so we will stick to such data. The code below will create a synthetic data set and split it into *training* and *test* data subsets, each with half of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c102125-4100-4424-b91d-7767d1b7918c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_moons(n_samples=1024, shuffle=True, noise=1/8, random_state=None)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f\"feature {i+1}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y+1, name=\"target\")\n",
    "y = \"label \" + y.astype(str)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b13dcb-1a8f-4b51-900f-3a20a6115934",
   "metadata": {},
   "source": [
    "As a first step, we can plot and look at the entire data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596a668-6fcd-4a30-a4a3-f8ac10e77ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6c0ec1-4ab6-4146-941f-587adea6ba3c",
   "metadata": {},
   "source": [
    "We can also plot just the data in our training data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35eb60-3848-4b92-9260-7396bf102e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b2ff71-e836-4814-af0a-38882b383356",
   "metadata": {},
   "source": [
    "Or just the data in our test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78507f2-2194-4bd5-99dd-18e1acf6a1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f74978-39c7-44e1-82af-3642803eb574",
   "metadata": {},
   "source": [
    "## **1.1 *k*-Nearest Neighbours**\n",
    "\n",
    "For our first model, we will use the [*k*-Nearest Neighbours](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier) algorithm. It is a relatively simple matter to build and train our model; however, we are naively using the default hyper-parameters here, which may not be appropriate for our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc5e632-7bb8-437a-8864-d4189f2e660a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.neighbors.KNeighborsClassifier()\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0673fa98-d9b0-4a96-9346-786300c28da9",
   "metadata": {},
   "source": [
    "Nevertheless, let's see how our model performs. First, we can plot the inferred (predicted) class for each of our test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ee28d-47b7-41c6-9e63-e46c1bdf1f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(X_test.reset_index(drop=True), pd.Series(clf.predict(X_test), name=\"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa454d76-8d51-46c6-b219-fbc6bb90c9e7",
   "metadata": {},
   "source": [
    "And we can score our model based on *precision*, *recall*, and *f1-score*. The code below also plots a [*confusion matrix*](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db885701-2ba2-44b3-b772-f0c9edb18fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "display_report(X_test, y_test, clf, title=\"k-Nearest Neighbours Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba648b33-0b20-48d6-9f38-eb4a8bf537b1",
   "metadata": {},
   "source": [
    "It is nearly perfect. The *k*-Nearest Neighbours algorithm seems well-suited for this particular data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb903235-7239-49db-8c53-1927457c9820",
   "metadata": {},
   "source": [
    "## **1.2 Support-Vector Machines (SVM)**\n",
    "An alternative approach to solving the classification problem is using *Support Vector Machines* (SVM). Below, we fit an SVM with a simple linear kernel to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1f30c3-d42f-4951-a8b9-9c0f9e1d9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"linear\")\n",
    "clf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce36ba57-e302-41d9-b8e8-2c53223d2535",
   "metadata": {},
   "source": [
    "The code below plots the *decision function* for the SVM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338ff92-fa70-4188-bddb-3dc0ccb6a02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_decision_function(X_test, y_test, clf, title=\"SVM with linear kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431cc75-fb74-4373-96ac-eb4bc49287a6",
   "metadata": {},
   "source": [
    "You can see that the decision function separates the classes poorly. We can see this qualitatively in the plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97807f65-8753-4755-997d-684420dd4b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_joint(X_test.reset_index(drop=True), pd.Series(clf.predict(X_test), name=\"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ed0a5-5a1f-459c-8316-79c5e54d1819",
   "metadata": {},
   "source": [
    "Generating our performance report, we see this reflected quantitatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a90c33-f26f-465f-a5b8-6dea10b65531",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_report(X_test, y_test, clf, title=\"SVM with linear kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26801377-09a0-4108-ac4c-610ee32d9ba9",
   "metadata": {},
   "source": [
    "We can repeat the analysis using a higher-order polynomial basis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97bcf84-8b30-41bb-826e-5dd3352c4d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"poly\")\n",
    "clf.fit(X_train, y_train)\n",
    "plot_decision_function(X_test, y_test, clf, title=\"SVM with linear kernel\")\n",
    "display_report(X_test, y_test, clf, title=\"SVM with polynomial kernel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f68cfa7-a2eb-4f0a-9be8-8fb90f780281",
   "metadata": {},
   "source": [
    "The performance is better, but still not as good as the k-Nearest Neighbours model. We can also try using a Radial Basis Function kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dd9934-84e8-42e7-addc-24dcbd0e26df",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(kernel=\"rbf\")\n",
    "clf.fit(X_train, y_train)\n",
    "plot_decision_function(X_test, y_test, clf, title=\"SVM with linear kernel\")\n",
    "display_report(X_test, y_test, clf, title=\"SVM with RBF kernel\")\n",
    "plot_joint(X_test.reset_index(drop=True), pd.Series(clf.predict(X_test), name=\"target\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0754155f-cb5c-4a5d-a592-69a057587df1",
   "metadata": {},
   "source": [
    "## **1.3 Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53520d52-759b-4bda-a8ba-b4211d5b5359",
   "metadata": {},
   "source": [
    "As a final example, we compare our previous results with a Random Forest Classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42366c66-1615-4573-bc83-a62dd9bba8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "display_report(X_test, y_test, clf, title=\"Random Forest Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8439cd7-7935-4650-befa-02cf4d65fa2d",
   "metadata": {},
   "source": [
    "The Random Forest model has the best performance of all the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7636fe79-1e10-4f5a-b668-15e0097d91b2",
   "metadata": {},
   "source": [
    "## **1.4 Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248e2d23-5e50-46dc-8797-e232b9ffd129",
   "metadata": {},
   "source": [
    "Try replacing the first line of Section 1.1 with one of the lines below, and repeat the analysis using a different data set. Is the relative performance of the models the same for different data sets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ad70f5-0008-4650-a3dc-8b7ee52b4783",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_circles(n_samples=1024, noise=1/32)\n",
    "X, y = sklearn.datasets.make_blobs(n_samples=1024, centers=2, cluster_std=2)\n",
    "X, y = sklearn.datasets.make_gaussian_quantiles(n_samples=1024, n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e549bd-2b2e-4bec-b74a-47d7c64329b7",
   "metadata": {},
   "source": [
    "# **2. Classification (Unsupervised)**\n",
    "\n",
    "In unsupervised learning methods, class labels are unknown _a priori_. Unsupervised learning algorithms must independently discover patterns in the training data set. We will look at two unsupervised classification methods: a) K-Means Clustering, and b) Gaussian Mixture Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0ce819-541e-426e-ad4f-628113feaae0",
   "metadata": {},
   "source": [
    "# **2.1 Data**\n",
    "\n",
    "We will begin with a synthetic data set like the one we started with in Section 1. In this synthetic example, we know the true labels of our data, but this information cannot be used by the unsupervised methods.\n",
    "\n",
    "After completing this section, try using one of the alternative data sets from Section 1. When you do so, note that you can change the `centers` and `n_classes` keyword arguments of the `make_blobs()` and `make_gaussian_quantiles()` functions, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e530e654-aedd-4267-8a2a-b95b3f5889a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = sklearn.datasets.make_moons(n_samples=1024, shuffle=True, noise=1/8, random_state=None)\n",
    "\n",
    "X = pd.DataFrame(X, columns=[f\"feature {i+1}\" for i in range(X.shape[1])])\n",
    "y = pd.Series(y+1, name=\"target\")\n",
    "y = \"label \" + y.astype(str)\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.5)\n",
    "\n",
    "plot_joint(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae2c4b0-ad79-472b-a742-b83945bcf57e",
   "metadata": {},
   "source": [
    "Generally speaking, unsupervised learning methods need a way to determine how many unique classes are represented within the data. The simplest way to solve this problem is to assume a certain number of classes. This is the approach we will take. After running the examples in this section, try changing the value of `n_classes` below to see how it affects your analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9d9cce-cfa1-41a0-850d-8bf92b9cffd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc16499d-430d-4f0f-bdd1-c6ec75e2bfe8",
   "metadata": {},
   "source": [
    "## **2.1 K-Means Clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9751d18-380f-4a75-9be2-25e0402e59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.cluster.KMeans(n_clusters=n_classes)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "plot_joint(\n",
    "    X_test.reset_index(drop=True), \n",
    "    pd.Series(clf.predict(X_test), name=\"target\"),\n",
    "    nclasses=n_classes,\n",
    "    palette=\"colorblind\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5835c20-f9f6-4f82-afd9-f6969c81acda",
   "metadata": {},
   "source": [
    "## **2.2 Gaussian Mixture Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038d8a06-2512-448f-80dc-d8a26cf51392",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.mixture.GaussianMixture(n_components=n_classes)\n",
    "clf = clf.fit(X, y)\n",
    "plot_joint(\n",
    "    X_test.reset_index(drop=True), \n",
    "    pd.Series(clf.predict(X_test), name=\"target\"),\n",
    "    nclasses=n_classes,\n",
    "    palette=\"colorblind\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83f1ee5-8e8a-48c1-9815-a5750f5b7bc2",
   "metadata": {},
   "source": [
    "Evaluating the performance of these methods is made challenging by the lack of labeled test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44dd5c3-c574-4cba-a00f-9f67dc1236e1",
   "metadata": {},
   "source": [
    "# **3. Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1c9649-1a90-4b79-9c3e-a15755f93cd0",
   "metadata": {},
   "source": [
    "In this section on Regression, we will compare the performance of a Linear Regression model with an Artificial Neural Network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79871aec-37d1-47dd-a0de-c7e58027f449",
   "metadata": {},
   "source": [
    "## **3.1 Data**\n",
    "We will use a data set containing information taken from a census on housing attributes in California, USA (https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset). The following is an excerpt on the data set from the `scikit-learn` documentation.\n",
    "\n",
    "```\n",
    "This dataset was obtained from the StatLib repository. https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
    "\n",
    "The target variable is the median house value for California districts, expressed in hundreds of thousands of dollars ($100,000).\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "An household is a group of people residing within a home. Since the average number of rooms and bedrooms in this dataset are provided per household, these columns may take surpinsingly large values for block groups with few households and many empty houses, such as vacation resorts.\n",
    "\n",
    "It can be downloaded/loaded using the sklearn.datasets.fetch_california_housing function.\n",
    "\n",
    "Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions, Statistics and Probability Letters, 33 (1997) 291-297\n",
    "```\n",
    "\n",
    "The code below will load the data, split it into *training* and *test* data sets (80% and 20% of the data, respsectively)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9def3-f90b-4b8e-98a7-aff62b99d440",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sklearn.datasets.fetch_california_housing(as_frame=True)\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422c5ae2-24b2-413d-8d4e-0c070d061dee",
   "metadata": {},
   "source": [
    "As before, we will plot the entire data set, the training data, and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebefbe1-4f64-48d0-a8b5-7f2348ae643f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plot_pairs(X, y, title=\"All data\")\n",
    "fig, axes = plot_pairs(X_train, y_train, title=\"Training data\")\n",
    "fig, axes = plot_pairs(X_test, y_test, title=\"Test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7873a2e5-acac-43be-9315-59d1381cffb2",
   "metadata": {},
   "source": [
    "## **3.1 Linear Regression**\n",
    "\n",
    "Linear Regression models are likely the most familiar, and the simplest type of Linear Regression model uses the $\\ell_2$-minimizing hyperplane to predict the target variable from the explanatory variables.\n",
    "\n",
    "The code below fits a simple least-squares Linear Regression model to the data and plots a comparison of the True versus Predicted values for the test data set. The mean square error, $\\varepsilon$, is reported in the top right corner.\n",
    "\n",
    "Note that the least-square Linear Regression model is deterministic, and running the code below multiple times will produce identical output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b3e7cd-e2d0-46d1-b0b5-dab7294c104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.linear_model.LinearRegression()\n",
    "model.fit(X_train, y_train);\n",
    "\n",
    "fig, axes = plot_pairs(X_test, y_test, model=model, title=f\"Linear Regression Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b23cbd-185b-4d38-81a9-59e901b9534f",
   "metadata": {},
   "source": [
    "## **3.2 Artifical Neural Networks**\n",
    "\n",
    "Artificial Neural Networks (ANNs) are popular today for their ability to approximate highly non-linear relationships between variables. To achieve this, they use a sequence of connected layers of nodes that mimic the structure and function of nerouns and synapses in the human brain.\n",
    "\n",
    "The code below will create an ANN with a single hidden layer of nodes with [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation functions.\n",
    "\n",
    "Note that the model parameters for ANNs are stochastically determined, so running the code below multiple times will produce different output each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a91a76-f74e-4fda-bb52-ab2d0fbeef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (100,)\n",
    "activation         = \"relu\"\n",
    "\n",
    "model = sklearn.neural_network.MLPRegressor(\n",
    "    activation=activation, \n",
    "    hidden_layer_sizes=hidden_layer_sizes\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plot_pairs(X_test, y_test, model=model, title=\"Multi-Layer Perceptron Model\\n(ReLu activation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be5e671-24ec-444e-abf9-d4158ff69c9b",
   "metadata": {},
   "source": [
    "We see that the much simpler Linear Regression model actually outperforms the relatively complex ANN. We can, however, try a different activation function. The code below creates an ANN with hyperbolic tangent activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d409320c-19f9-4939-a250-d53ad2a20fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (100,)\n",
    "activation         = \"tanh\"\n",
    "\n",
    "model = sklearn.neural_network.MLPRegressor(\n",
    "    activation=activation,\n",
    "    hidden_layer_sizes=hidden_layer_sizes\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plot_pairs(X_test, y_test, model=model, title=\"Multi-Layer Perceptron Model\\n(tanh activation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bfad4a-5713-4ab4-849b-9f51bd5527cc",
   "metadata": {},
   "source": [
    "The tanh activation function outperforms the ReLu activation function in this application.\n",
    "\n",
    "We can also increase the number of hidden layers and/or the size of each hidden layer. The code below creates an ANN with two hidden layers (one with 128 nodes and one with 64 nodes) using hyperbolic tanget activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd398c4f-472f-4ce9-b18a-aa1485b3208b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (128, 64)\n",
    "activation         = \"tanh\"\n",
    "\n",
    "model = sklearn.neural_network.MLPRegressor(\n",
    "    activation=activation,\n",
    "    hidden_layer_sizes=hidden_layer_sizes\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plot_pairs(X_test, y_test, model=model, title=\"Multi-Layer Perceptron Model\\n(tanh activation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429006d4-6621-4c6c-9275-873428e72cdb",
   "metadata": {},
   "source": [
    "Does this more complex ANN outperform the smaller ANN with just a single layer of 100 nodes?\n",
    "\n",
    "\n",
    "## **3.3 Exercise**\n",
    "Try changing the number and size of hidden layers and the activation function in the code below to see if you can build a better ANN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac06cb56-9202-40e0-908d-37d2e06b6b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = (128, 64, 128)\n",
    "activation         = \"tanh\" # Try \"tanh\", \"relu\", and \"logistic\"\n",
    "\n",
    "model = sklearn.neural_network.MLPRegressor(\n",
    "    activation=activation,\n",
    "    hidden_layer_sizes=hidden_layer_sizes\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "fig, axes = plot_pairs(\n",
    "    X_test, \n",
    "    y_test, \n",
    "    model=model, \n",
    "    title=f\"Multi-Layer Perceptron Model\\n\"\n",
    "    f\"{activation} activation\\n\"\n",
    "    f\"Hidden layer sizes: {hidden_layer_sizes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6935a02d-1b41-4b4c-a92f-a0b124f3e4e0",
   "metadata": {},
   "source": [
    "# **4. Image Recognition**\n",
    "\n",
    "Finally, we will build a facial recognition model.\n",
    "\n",
    "## **4.1 Data**\n",
    "\n",
    "For this, we will use the [\"Labeled Faces in the Wild\"](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html) data set, which contains images of faces of several well-known world leaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f69714-4093-4353-b02f-449afe0b88be",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces = sklearn.datasets.fetch_lfw_people(min_faces_per_person=70, resize=0.4)\n",
    "X = faces[\"data\"]\n",
    "y = faces[\"target_names\"][faces[\"target\"]]\n",
    "n_samples, h, w = faces[\"images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd692c2-96f3-4cf6-9e64-9ddc806eb9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gallery(X, h, w, titles=y, nrows=4, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720bca22-6972-497b-b8fe-da7e8f03390a",
   "metadata": {},
   "source": [
    "Next we split the data in training and test data sets. The test data set comprises 25% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cc61b5-19b8-4000-8281-ceb2ed21b4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fe96f-401b-4ffb-b737-fbb05f9b770e",
   "metadata": {},
   "source": [
    "## **4.2 Dimensionality Reduction: Principal Component Analysis (PCA)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a35778-b314-4d3b-882d-bb596f70df4a",
   "metadata": {},
   "source": [
    "First, we need to extract features to represent each image. This can be achieved using Principal Component Analysis to reduce the dimensionality of the input data. We will reduce each image to a set of 128 coefficients, each of which is associated with an *eigenface*. Each of the original images can be approximately reconstructed by multiplying the eigenfaces by their respective coefficients and summing. The eigenfaces represent an orthogonal basis that spans (within some error) the space of faces in the input.\n",
    "\n",
    "The code below will compute and plot (a portion of) the set of eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65340f4e-c108-4603-a2d6-4ac35dbaa0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components=128\n",
    "\n",
    "pca = sklearn.decomposition.PCA(\n",
    "    n_components=n_components, \n",
    "    svd_solver=\"randomized\",\n",
    "    whiten=True\n",
    ")\n",
    "pca.fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "\n",
    "plot_gallery(eigenfaces, h, w, nrows=7, ncols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ef7437-152d-46dc-bcdd-6ccf03e22dac",
   "metadata": {},
   "source": [
    "Having computed the spanning set of eigenfaces, we can decompose the training and test data as linear combinations of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e536f1-5975-4338-8619-2944bdaeee0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_train = pca.transform(X_train)\n",
    "W_test  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32776260-a9fe-45cf-a025-d807b2a08db6",
   "metadata": {},
   "source": [
    "Each image is now represented by a vector of 128 eigenface coefficients.\n",
    "\n",
    "Using an SVM with a RBF kernel as before, we can build our classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae31dfde-c532-4379-820e-08c9ab743bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = sklearn.svm.SVC(kernel='rbf', class_weight='balanced')\n",
    "clf.fit(W_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df0aeea-8430-46c5-a5ab-06385dc3f729",
   "metadata": {},
   "source": [
    "We can visually inspect our inferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610e6690-881b-49cb-a59e-c4857e571ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(W_test)\n",
    "plot_gallery(X_test, h, w, titles=y_pred, nrows=4, ncols=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bed6d3-a873-4091-a5ab-e86ac4943f3c",
   "metadata": {},
   "source": [
    "And finally, we can display a performance report for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cb86fb-7094-4af5-a2fa-c39cd1680987",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_report(W_test, y_test, clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5b74c2-3777-4143-a31f-f102b94256f7",
   "metadata": {},
   "source": [
    "How did the model perform?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py39]",
   "language": "python",
   "name": "conda-env-py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
